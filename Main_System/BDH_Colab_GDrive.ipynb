{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BDH Model Training on Google Colab with Google Drive\n",
        "\n",
        "This notebook trains the BDH model on novels from Google Drive and uses train.csv for labels.\n",
        "\n",
        "**Steps:**\n",
        "1. Mount Google Drive\n",
        "2. Load files from Drive\n",
        "3. Train the model\n",
        "4. Download the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch numpy tqdm google-api-python-client google-auth-httplib2 google-auth-oauthlib -q\n",
        "print(\"âœ“ Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ“ Google Drive mounted\")\n",
        "\n",
        "# Set your Google Drive folder path here\n",
        "# Example: '/content/drive/MyDrive/KDSH26' or '/content/drive/MyDrive/Books'\n",
        "GDRIVE_FOLDER_PATH = '/content/drive/MyDrive/KDSH26'  # â¬…ï¸ CHANGE THIS to your folder path\n",
        "\n",
        "print(f\"\\nðŸ“ Using folder: {GDRIVE_FOLDER_PATH}\")\n",
        "print(\"   Make sure this folder contains:\")\n",
        "print(\"   - Your novel .txt files\")\n",
        "print(\"   - train.csv\")\n",
        "print(\"   - test.csv (optional)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Core Files\n",
        "\n",
        "Upload these Python files from your local machine:\n",
        "- `core/bdh.py`\n",
        "- `utils/bdh_narrative_builder.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create directory structure\n",
        "os.makedirs('core', exist_ok=True)\n",
        "os.makedirs('utils', exist_ok=True)\n",
        "os.makedirs('Books', exist_ok=True)\n",
        "\n",
        "print(\"ðŸ“ Please upload these Python files:\")\n",
        "print(\"1. bdh.py (from core/bdh.py)\")\n",
        "print(\"2. bdh_narrative_builder.py (from utils/bdh_narrative_builder.py)\")\n",
        "print(\"\\nâš ï¸  IMPORTANT: Click 'Choose Files' and select BOTH files at once!\")\n",
        "print(\"   Hold Ctrl (Windows) or Cmd (Mac) to select multiple files\\n\")\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(f\"\\nðŸ“¦ Uploaded {len(uploaded)} file(s):\")\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"   - {filename}\")\n",
        "\n",
        "# Organize uploaded files\n",
        "print(\"\\nðŸ“‚ Organizing files...\")\n",
        "moved_bdh = False\n",
        "moved_builder = False\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    # Handle bdh.py (could be named bdh.py or core/bdh.py)\n",
        "    if filename == 'bdh.py' or filename.endswith('/bdh.py') or 'bdh.py' in filename:\n",
        "        target_path = 'core/bdh.py'\n",
        "        if os.path.exists(filename):\n",
        "            os.rename(filename, target_path)\n",
        "            print(f\"âœ“ Moved {filename} â†’ {target_path}\")\n",
        "            moved_bdh = True\n",
        "    # Handle bdh_narrative_builder.py\n",
        "    elif 'bdh_narrative_builder.py' in filename:\n",
        "        target_path = 'utils/bdh_narrative_builder.py'\n",
        "        if os.path.exists(filename):\n",
        "            os.rename(filename, target_path)\n",
        "            print(f\"âœ“ Moved {filename} â†’ {target_path}\")\n",
        "            moved_builder = True\n",
        "\n",
        "# Check if files are in place\n",
        "print(\"\\nâœ… Verification:\")\n",
        "if os.path.exists('core/bdh.py'):\n",
        "    print(\"   âœ“ core/bdh.py exists\")\n",
        "else:\n",
        "    print(\"   âœ— core/bdh.py MISSING - please upload bdh.py\")\n",
        "\n",
        "if os.path.exists('utils/bdh_narrative_builder.py'):\n",
        "    print(\"   âœ“ utils/bdh_narrative_builder.py exists\")\n",
        "else:\n",
        "    print(\"   âœ— utils/bdh_narrative_builder.py MISSING - please upload bdh_narrative_builder.py\")\n",
        "\n",
        "if moved_bdh and moved_builder:\n",
        "    print(\"\\nðŸŽ‰ Both files uploaded successfully!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Some files are missing. Please run this cell again and upload both files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Books and CSV from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Add paths\n",
        "sys.path.insert(0, 'core')\n",
        "sys.path.insert(0, 'utils')\n",
        "\n",
        "# Copy books from Google Drive to local Books folder\n",
        "print(\"ðŸ“š Loading books from Google Drive...\")\n",
        "books = []\n",
        "book_files = list(Path(GDRIVE_FOLDER_PATH).glob(\"*.txt\"))\n",
        "\n",
        "if not book_files:\n",
        "    print(f\"âš ï¸  No .txt files found in {GDRIVE_FOLDER_PATH}\")\n",
        "    print(\"   Please check the folder path above!\")\n",
        "else:\n",
        "    for book_file in book_files:\n",
        "        print(f\"  Loading {book_file.name}...\")\n",
        "        # Copy to local Books folder\n",
        "        shutil.copy(book_file, f'Books/{book_file.name}')\n",
        "        # Read content\n",
        "        with open(book_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            text = f.read()\n",
        "            books.append(text)\n",
        "            print(f\"    âœ“ Loaded {len(text):,} characters\")\n",
        "\n",
        "print(f\"\\nâœ… Loaded {len(books)} novels\")\n",
        "\n",
        "# Load train.csv from Google Drive\n",
        "print(\"\\nðŸ“‹ Loading train.csv from Google Drive...\")\n",
        "train_csv_path = Path(GDRIVE_FOLDER_PATH) / \"train.csv\"\n",
        "if train_csv_path.exists():\n",
        "    shutil.copy(train_csv_path, 'train.csv')\n",
        "    print(\"âœ“ train.csv loaded\")\n",
        "else:\n",
        "    print(\"âš ï¸  train.csv not found in Google Drive folder\")\n",
        "\n",
        "# Load test.csv (optional)\n",
        "test_csv_path = Path(GDRIVE_FOLDER_PATH) / \"test.csv\"\n",
        "if test_csv_path.exists():\n",
        "    shutil.copy(test_csv_path, 'test.csv')\n",
        "    print(\"âœ“ test.csv loaded\")\n",
        "else:\n",
        "    print(\"âš ï¸  test.csv not found (optional)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from bdh import BDH, BDHConfig\n",
        "from bdh_narrative_builder import _text_to_tokens\n",
        "\n",
        "print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
        "print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"âœ“ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train Model\n",
        "\n",
        "This uses **MEMORY-EFFICIENT** training code to prevent crashes:\n",
        "- âœ… Mixed precision training - **1.5-2x faster + less memory**\n",
        "- âœ… `torch.compile()` - **2-3x faster** (optional, can disable if crashes)\n",
        "- âœ… Random batches (no pre-allocation)\n",
        "- âœ… CUDA optimizations\n",
        "- âœ… Memory cleanup every 500 steps\n",
        "\n",
        "**Training time: 20-35 minutes** on Colab GPU\n",
        "\n",
        "**Settings (memory-safe):**\n",
        "- `max_iters=3000` iterations\n",
        "- `batch_size=16` (reduced to prevent OOM)\n",
        "- `block_size=256` (reduced to prevent OOM)\n",
        "- Token limit: 1M (prevents crashes)\n",
        "\n",
        "**If session crashes:**\n",
        "1. Reduce `batch_size=8` in the code below\n",
        "2. Reduce `block_size=128`\n",
        "3. Set `USE_COMPILE = False` in the function\n",
        "4. Reduce `max_iters=2000` for faster completion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copyright Pathway Technology, Inc.\n",
        "# Adapted to use books from Google Drive instead of Shakespeare dataset\n",
        "\n",
        "import os\n",
        "from contextlib import nullcontext\n",
        "import bdh\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dtype = (\n",
        "    \"bfloat16\"\n",
        "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    else \"float16\"\n",
        ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "\n",
        "ptdtype = {\n",
        "    \"float32\": torch.float32,\n",
        "    \"bfloat16\": torch.bfloat16,\n",
        "    \"float16\": torch.float16,\n",
        "}[dtype]\n",
        "\n",
        "ctx = (\n",
        "    torch.amp.autocast(device_type=device.type, dtype=ptdtype)\n",
        "    if \"cuda\" in device.type\n",
        "    else nullcontext()\n",
        ")\n",
        "\n",
        "scaler = torch.amp.GradScaler(device=device.type, enabled=(dtype == \"float16\"))\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
        "\n",
        "print(f\"Using device: {device} with dtype {dtype}\")\n",
        "\n",
        "# Configuration\n",
        "BDH_CONFIG = bdh.BDHConfig()\n",
        "BLOCK_SIZE = 512\n",
        "BATCH_SIZE = 16  # Reduced from 32 to prevent OOM crashes\n",
        "MAX_ITERS = 3000\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 0.1\n",
        "LOG_FREQ = 100\n",
        "\n",
        "# If you still get OOM errors, reduce these:\n",
        "# BATCH_SIZE = 8\n",
        "# BLOCK_SIZE = 256\n",
        "\n",
        "# Prepare input file from books (instead of fetch_data())\n",
        "print(\"Preparing training data from books...\")\n",
        "combined_text = \"\\n\\n\".join(books)\n",
        "temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.txt')\n",
        "temp_file.write(combined_text.encode('utf-8'))\n",
        "temp_file.close()\n",
        "input_file_path = temp_file.name\n",
        "print(f\"Training data prepared: {len(combined_text):,} characters\")\n",
        "\n",
        "def get_batch(split):\n",
        "    # treat the file as bytes\n",
        "    data = np.memmap(input_file_path, dtype=np.uint8, mode=\"r\")\n",
        "    if split == \"train\":\n",
        "        data = data[: int(0.9 * len(data))]\n",
        "    else:\n",
        "        data = data[int(0.9 * len(data)) :]\n",
        "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "    x = torch.stack(\n",
        "        [torch.from_numpy((data[i : i + BLOCK_SIZE]).astype(np.int64)) for i in ix]\n",
        "    )\n",
        "    y = torch.stack(\n",
        "        [\n",
        "            torch.from_numpy((data[i + 1 : i + 1 + BLOCK_SIZE]).astype(np.int64))\n",
        "            for i in ix\n",
        "        ]\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(\n",
        "            device, non_blocking=True\n",
        "        )\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Training (EXACTLY like original)\n",
        "model = bdh.BDH(BDH_CONFIG).to(device)\n",
        "\n",
        "# torch.compile can cause crashes on some Colab GPUs - make it optional\n",
        "try:\n",
        "    print(\"Compiling model (this may take 1-2 minutes)...\")\n",
        "    model = torch.compile(model)\n",
        "    print(\"âœ“ Model compiled successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Compilation failed: {e}\")\n",
        "    print(\"Continuing without compilation (slower but stable)\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "x, y = get_batch(\"train\")\n",
        "loss_acc = 0\n",
        "loss_steps = 0\n",
        "\n",
        "try:\n",
        "    for step in range(MAX_ITERS):\n",
        "        with ctx:\n",
        "            logits, loss = model(x, y)\n",
        "        x, y = get_batch(\"train\")\n",
        "        loss_acc += loss\n",
        "        loss_steps += 1\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % LOG_FREQ == 0:\n",
        "            print(f\"Step: {step}/{MAX_ITERS} loss {loss_acc.item() / loss_steps:.3}\")\n",
        "            loss_acc = 0\n",
        "            loss_steps = 0\n",
        "        \n",
        "        # Periodic memory cleanup\n",
        "        if step % 500 == 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e).lower():\n",
        "        print(\"\\nâŒ GPU Out of Memory!\")\n",
        "        print(\"Solutions:\")\n",
        "        print(\"1. Reduce BATCH_SIZE to 8\")\n",
        "        print(\"2. Reduce BLOCK_SIZE to 256\")\n",
        "        print(\"3. Use Runtime > Change runtime type > GPU > T4 (if available)\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "print(\"Training done, now generating a sample \")\n",
        "model.eval()\n",
        "prompt = torch.tensor(\n",
        "    bytearray(\"To be or \", \"utf-8\"), dtype=torch.long, device=device\n",
        ").unsqueeze(0)\n",
        "ret = model.generate(prompt, max_new_tokens=100, top_k=3)\n",
        "ret_decoded = bytes(ret.to(torch.uint8).to(\"cpu\").squeeze(0)).decode(\n",
        "    errors=\"backslashreplace\"\n",
        ")\n",
        "print(ret_decoded)\n",
        "\n",
        "# Cleanup temp file\n",
        "try:\n",
        "    os.unlink(input_file_path)\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "model_path = 'models/bdh_trained.pt'\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'config': config,\n",
        "    'epochs': 50\n",
        "}, model_path)\n",
        "\n",
        "print(f\"âœ… Model saved to {model_path}\")\n",
        "print(f\"   File size: {os.path.getsize(model_path) / 1e6:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Download Model\n",
        "\n",
        "Download the trained model to your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"ðŸ“¥ Downloading trained model...\")\n",
        "files.download(model_path)\n",
        "print(\"\\nâœ… Download complete!\")\n",
        "print(\"\\nðŸ“‹ Next steps:\")\n",
        "print(\"1. Move the downloaded file to: Main_System/scripts/models/bdh_trained.pt\")\n",
        "print(\"2. Run: cd Main_System/scripts && python3 bdh_quickstart.py\")\n",
        "print(\"3. The script will automatically use the trained model!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
